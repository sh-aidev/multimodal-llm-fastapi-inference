<div align="center">

# Multimodal Large Language Model(LLAVA/BakLLAVA) with FastApi Inference

[![python](https://img.shields.io/badge/-Python_%7C_3.10-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
![license](https://img.shields.io/badge/License-MIT-green?logo=mit&logoColor=white)

This repository contains the inference code for the multimodal large language model(LLAVA/BakLLAVA) with FastApi backend. The model is based on the [BakLLAVA](https://huggingface.co/llava-hf/bakLlava-v1-hf). FastAPI backend is written in a format such that directly we can call openai api and get the response.
</div>

## ğŸ“Œ Feature
- [x] Multimodal Large Language Model(LLAVA/BakLLAVA) Inference
- [x] FastApi Backend
- [x] OpenAI like API

## ğŸ“  Project Structure
The directory structure of new project looks like this:
```bash
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ __main__.py
â”œâ”€â”€ configs
â”‚   â””â”€â”€ config.toml
â”œâ”€â”€ logs
â”œâ”€â”€ requirements.txt
â””â”€â”€ src
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ app.py
    â”œâ”€â”€ core
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ multimodal_llm_inference.py
    â”œâ”€â”€ pylogger
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ logger.py
    â”œâ”€â”€ server
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ client.py
    â”‚   â”œâ”€â”€ router.py
    â”‚   â””â”€â”€ server.py
    â””â”€â”€ utils
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ config.py
        â”œâ”€â”€ logger.py
        â”œâ”€â”€ models.py
        â”œâ”€â”€ openai_protocol.py
        â””â”€â”€ textformat.py

```

## ğŸš€ Getting Started

### Step 1: Clone the repository
```bash
git clone https://github.com/sh-aidev/bakLlava-chat-vision.git
cd bakLlava-chat-vision
```

### Step 2: Install the required dependencies
```bash
python3 -m pip install -r requirements.txt
```

### Step 3: Run the FastAPI server
```bash
python3 __main__.py
```

### Step 4: To test the FastAPI server with openai library
```bash
python3 src/server/client.py
```

NOTE: to run the frontend code, please refer to the [frontend](https://github.com/sh-aidev/bakLlava-chat-vision.git) repository.
## ğŸ“œ  References
- [BakLLAVA](https://huggingface.co/llava-hf/bakLlava-v1-hf)
- [FastAPI](https://fastapi.tiangolo.com/)
- [OpenAI](https://beta.openai.com/docs/)
- [Uvicorn](https://www.uvicorn.org/)